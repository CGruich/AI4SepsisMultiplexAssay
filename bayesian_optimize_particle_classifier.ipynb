{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "281a6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc408f27",
   "metadata": {},
   "source": [
    "# Bayesian Optimize Training Particle Classifier(s): Control Panel\n",
    "### Author: CG\n",
    "\n",
    "This notebook succeeds the \"train_particle_classifiers.ipynb\" notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee14e8",
   "metadata": {},
   "source": [
    "Here, we define a dictionary of input variables to initiate our particle detector training pipeline. This dictionary will be saved as a .json file and passed to the pipeline execution script, main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea25ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDict = {\n",
    "    \"sample_parent_directory\": \"/home/cameron/Dropbox (University of Michigan)/DL_training/data/classifier_training_samples\",\n",
    "    # Where to save the timestamped model\n",
    "    \"model_save_parent_directory\": \"data/models/code\",\n",
    "    # Specify codes to separately train the region detector on\n",
    "    # Must be in format \"(NUMBER)\" as per our experimental convention for naming codes\n",
    "    \"code_list\": [\"(1)\", \"(2)\", \"(3)\", \"(4)\", \"(5)\", \"(6)\"],\n",
    "    # Fraction of total training dataset to allocate as the test dataset for model predictive ability\n",
    "    \"test_size\": 0.20,\n",
    "    # Log via TensorBoard\n",
    "    \"log\": False,\n",
    "    # Printing verbosity\n",
    "    \"verbose\": False,\n",
    "    # Timestamp corresponding to this particular hyperparameter optimization run,\n",
    "    \"timestamp\": datetime.now().strftime(\"%m_%d_%y_%H:%M\") + \"_production\",\n",
    "    \"strat_kfold\": {\n",
    "        \"activate\": True,\n",
    "        # Num. of folds\n",
    "        \"num_folds\": 5,\n",
    "        # Controls the splitting of data in a reproducible way if the same seed is used\n",
    "        \"random_state\": 100,\n",
    "        # Stratify by particle barcode stain level instead of just particle barcode\n",
    "        \"stratify_by_stain\": True,\n",
    "    },\n",
    "    # Save hyperparameter results every 'save_every' trials\n",
    "    \"save_every_n\": 2,\n",
    "    # Place to save checkpointed files\n",
    "    \"checkpoint_path\": \"hpo/code_classifier/multi/\",\n",
    "    # Load a pre-saved checkpoint to continue a study\n",
    "    # Defaults to None if no checkpoint is specified\n",
    "    \"checkpoint\": '/home/cameron/Dropbox (University of Michigan)/DL_training/hpo/code_classifier/multi/11_28_23_23:33_production/ckpt_16.pkl',\n",
    "    # Number of hyperparameter trials to try\n",
    "    \"num_hpo\": 60,\n",
    "    # Time in seconds to wait before the study\n",
    "    \"timeout\": None,\n",
    "    # How many epochs of no improvement to wait before stopping training run\n",
    "    \"patience\": 7500,\n",
    "    # How many epochs to wait before starting early-stopping\n",
    "    \"warmup\": 200,\n",
    "}\n",
    "\n",
    "# File save name\n",
    "inputJSON = \"bayesian_train_particle_detector.json\"\n",
    "# Save\n",
    "with open(inputJSON, \"w\") as jsonFile:\n",
    "    json.dump(inputDict, jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9cbd7a",
   "metadata": {},
   "source": [
    "## Train Particle Detectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0603d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-11-29 13:49:48,615]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-29 16:37:00,657]\u001b[0m Trial 37 finished with value: 95.58140563964844 and parameters: {'learning_rate': 5.843479192450395e-05, 'batch_size': 110, 'fully_connected_size': 576, 'fully_connected_layers': 5, 'dropout_rate': 0.052645899569034434, 'weight_decay': 0.028255488688593695}. Best is trial 18 with value: 99.09648284912109.\u001b[0m\n",
      "\u001b[32m[I 2023-11-29 16:51:46,555]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-29 17:27:02,315]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-29 19:25:29,866]\u001b[0m Trial 40 finished with value: 95.88090362548829 and parameters: {'learning_rate': 9.439911757825264e-05, 'batch_size': 120, 'fully_connected_size': 448, 'fully_connected_layers': 5, 'dropout_rate': 0.007195281048523748, 'weight_decay': 0.06125196732280815}. Best is trial 18 with value: 99.09648284912109.\u001b[0m\n",
      "\u001b[32m[I 2023-11-29 19:37:18,060]\u001b[0m Trial 41 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-29 20:52:07,564]\u001b[0m Trial 42 finished with value: 77.40402069091797 and parameters: {'learning_rate': 2.6847934626507896e-05, 'batch_size': 190, 'fully_connected_size': 768, 'fully_connected_layers': 4, 'dropout_rate': 0.7752586752939639, 'weight_decay': 0.10894930887421235}. Best is trial 18 with value: 99.09648284912109.\u001b[0m\n",
      "\u001b[32m[I 2023-11-29 21:12:10,463]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-29 21:32:02,595]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-29 21:49:49,984]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-29 22:13:27,988]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-29 23:01:04,969]\u001b[0m Trial 47 finished with value: 82.12411804199219 and parameters: {'learning_rate': 0.0010835827900367934, 'batch_size': 160, 'fully_connected_size': 896, 'fully_connected_layers': 1, 'dropout_rate': 0.5529561607120995, 'weight_decay': 0.046046664806589414}. Best is trial 18 with value: 99.09648284912109.\u001b[0m\n",
      "\u001b[32m[I 2023-11-29 23:22:44,970]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-30 02:02:33,058]\u001b[0m Trial 49 finished with value: 93.06683502197265 and parameters: {'learning_rate': 8.254216432716999e-05, 'batch_size': 110, 'fully_connected_size': 640, 'fully_connected_layers': 5, 'dropout_rate': 0.06264908483949379, 'weight_decay': 0.03343717354386877}. Best is trial 18 with value: 99.09648284912109.\u001b[0m\n",
      "\u001b[32m[I 2023-11-30 02:22:41,291]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-30 02:53:55,685]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-30 03:38:34,177]\u001b[0m Trial 52 finished with value: 80.8190933227539 and parameters: {'learning_rate': 0.0021988490097744496, 'batch_size': 200, 'fully_connected_size': 640, 'fully_connected_layers': 1, 'dropout_rate': 0.4983188020328299, 'weight_decay': 0.039352756446782336}. Best is trial 18 with value: 99.09648284912109.\u001b[0m\n",
      "\u001b[32m[I 2023-11-30 04:02:50,704]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-30 04:17:27,890]\u001b[0m Trial 54 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-30 04:51:47,644]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-30 05:59:37,123]\u001b[0m Trial 56 finished with value: 80.61859130859375 and parameters: {'learning_rate': 0.0011338826924568207, 'batch_size': 160, 'fully_connected_size': 896, 'fully_connected_layers': 3, 'dropout_rate': 0.7932783050287752, 'weight_decay': 0.002662183431188808}. Best is trial 18 with value: 99.09648284912109.\u001b[0m\n",
      "\u001b[32m[I 2023-11-30 06:11:34,000]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-30 06:45:30,459]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-30 07:40:44,631]\u001b[0m Trial 59 finished with value: 80.21708374023437 and parameters: {'learning_rate': 0.0008709548293384218, 'batch_size': 150, 'fully_connected_size': 896, 'fully_connected_layers': 1, 'dropout_rate': 0.7083221255871047, 'weight_decay': 0.044008274269755954}. Best is trial 18 with value: 99.09648284912109.\u001b[0m\n",
      "\u001b[32m[I 2023-11-30 07:56:44,468]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-30 08:21:13,501]\u001b[0m Trial 61 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-30 08:35:06,414]\u001b[0m Trial 62 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-11-30 08:49:46,602]\u001b[0m Trial 63 pruned. \u001b[0m\n",
      "\u001b[33m[W 2023-11-30 09:38:21,469]\u001b[0m Trial 64 failed with parameters: {'learning_rate': 7.006586074891205e-05, 'batch_size': 50, 'fully_connected_size': 960, 'fully_connected_layers': 3, 'dropout_rate': 0.6610793495164873, 'weight_decay': 0.0226465469483307} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/action_functions.py\", line 809, in objective_with_custom_input\n",
      "    return bayesian.objective_code_classifier(trial, pipeline_inputs)\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/bayesian.py\", line 31, in objective_code_classifier\n",
      "    cross_val_scores = action_functions.train_code_classifier(\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/action_functions.py\", line 756, in train_code_classifier\n",
      "    cross_val_scores = trainer.train(\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/model_training/code_classification_trainer_GPU.py\", line 250, in train\n",
      "    optimizer.step()\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 140, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 23, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/optim/adam.py\", line 234, in step\n",
      "    adam(params_with_grad,\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/optim/adam.py\", line 300, in adam\n",
      "    func(params,\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/optim/adam.py\", line 354, in _single_tensor_adam\n",
      "    grad = grad.add(param, alpha=weight_decay)\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-11-30 09:38:21,477]\u001b[0m Trial 64 failed with value None.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/main_cg_GPU.py\", line 42, in <module>\n",
      "    action_functions.bayesian_optimize_code_classifer(pipeline_inputs=pipeline_inputs)\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/action_functions.py\", line 811, in bayesian_optimize_code_classifer\n",
      "    study = bayesian.checkpoint_study(\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/bayesian.py\", line 83, in checkpoint_study\n",
      "    study.optimize(objective_function, n_trials=1)\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/optuna/study/study.py\", line 425, in optimize\n",
      "    _optimize(\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 66, in _optimize\n",
      "    _optimize_sequential(\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 163, in _optimize_sequential\n",
      "    frozen_trial = _run_trial(study, func, catch)\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 251, in _run_trial\n",
      "    raise func_err\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/action_functions.py\", line 809, in objective_with_custom_input\n",
      "    return bayesian.objective_code_classifier(trial, pipeline_inputs)\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/bayesian.py\", line 31, in objective_code_classifier\n",
      "    cross_val_scores = action_functions.train_code_classifier(\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/action_functions.py\", line 756, in train_code_classifier\n",
      "    cross_val_scores = trainer.train(\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/model_training/code_classification_trainer_GPU.py\", line 250, in train\n",
      "    optimizer.step()\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 140, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 23, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/optim/adam.py\", line 234, in step\n",
      "    adam(params_with_grad,\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/optim/adam.py\", line 300, in adam\n",
      "    func(params,\n",
      "  File \"/home/cameron/Documents/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/optim/adam.py\", line 354, in _single_tensor_adam\n",
      "    grad = grad.add(param, alpha=weight_decay)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\n",
    "    f\"python main_cg_GPU.py --action bayesian_optimize_code_classifier --pipeline_inputs {inputJSON}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
