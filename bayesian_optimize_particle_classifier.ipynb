{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281a6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc408f27",
   "metadata": {},
   "source": [
    "# Bayesian Optimize Training Particle Classifier(s): Control Panel\n",
    "### Author: CG\n",
    "\n",
    "This notebook succeeds the \"train_particle_classifiers.ipynb\" notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee14e8",
   "metadata": {},
   "source": [
    "Here, we define a dictionary of input variables to initiate our particle detector training pipeline. This dictionary will be saved as a .json file and passed to the pipeline execution script, main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea25ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDict = {\n",
    "    \"sample_parent_directory\": \"/home/cameron/Dropbox (University of Michigan)/DL_training/data/classifier_training_samples\",\n",
    "    # Where to save the timestamped model\n",
    "    \"model_save_parent_directory\": \"data/models/code\",\n",
    "    # Specify codes to separately train the region detector on\n",
    "    # Must be in format \"(NUMBER)\" as per our experimental convention for naming codes\n",
    "    \"code_list\": [\"(1)\", \"(2)\", \"(3)\", \"(4)\", \"(5)\"],\n",
    "    # Fraction of total training dataset to allocate as the test dataset for model predictive ability\n",
    "    \"test_size\": 0.20,\n",
    "    # Log via TensorBoard\n",
    "    \"log\": False,\n",
    "    # Printing verbosity\n",
    "    \"verbose\": False,\n",
    "    # Timestamp corresponding to this particular hyperparameter optimization run,\n",
    "    \"timestamp\": datetime.now().strftime(\"%m_%d_%y_%H:%M\") + \"_production_laptop\",\n",
    "    \"strat_kfold\": {\n",
    "        \"activate\": True,\n",
    "        # Num. of folds\n",
    "        \"num_folds\": 5,\n",
    "        # Controls the splitting of data in a reproducible way if the same seed is used\n",
    "        \"random_state\": 100,\n",
    "        # Stratify by particle barcode stain level instead of just particle barcode\n",
    "        \"stratify_by_stain\": True,\n",
    "    },\n",
    "    # Save hyperparameter results every 'save_every' trials\n",
    "    \"save_every_n\": 2,\n",
    "    # Place to save checkpointed files\n",
    "    \"checkpoint_path\": \"hpo/code_classifier/multi/\",\n",
    "    # Load a pre-saved checkpoint to continue a study\n",
    "    # Defaults to None if no checkpoint is specified\n",
    "    \"checkpoint\": None,\n",
    "    # Number of hyperparameter trials to try\n",
    "    \"num_hpo\": 60,\n",
    "    # Time in seconds to wait before the study\n",
    "    \"timeout\": None,\n",
    "    # How many epochs of no improvement to wait before stopping training run\n",
    "    \"patience\": 7500,\n",
    "    # How many epochs to wait before starting early-stopping\n",
    "    \"warmup\": 200,\n",
    "}\n",
    "\n",
    "# File save name\n",
    "inputJSON = \"bayesian_train_particle_detector.json\"\n",
    "# Save\n",
    "with open(inputJSON, \"w\") as jsonFile:\n",
    "    json.dump(inputDict, jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9cbd7a",
   "metadata": {},
   "source": [
    "## Train Particle Detectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0603d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-01-24 15:05:56,426]\u001b[0m A new study created in memory with name: no-name-60809689-25fc-40a3-a82a-919b583879d1\u001b[0m\n",
      "\u001b[32m[I 2024-01-24 16:54:01,635]\u001b[0m Trial 0 finished with value: 77.68909912109375 and parameters: {'learning_rate': 0.005434053983860237, 'batch_size': 70, 'fully_connected_size': 512, 'fully_connected_layers': 5, 'dropout_rate': 0.003775084952778052, 'weight_decay': 0.04862764831324569}. Best is trial 0 with value: 77.68909912109375.\u001b[0m\n",
      "\u001b[32m[I 2024-01-24 17:46:47,857]\u001b[0m Trial 1 finished with value: 41.13640594482422 and parameters: {'learning_rate': 0.006707494139776939, 'batch_size': 170, 'fully_connected_size': 256, 'fully_connected_layers': 3, 'dropout_rate': 0.7130575634498113, 'weight_decay': 0.08368084884687584}. Best is trial 0 with value: 77.68909912109375.\u001b[0m\n",
      "\u001b[32m[I 2024-01-24 19:48:42,954]\u001b[0m Trial 2 finished with value: 49.86305160522461 and parameters: {'learning_rate': 0.001853290342218555, 'batch_size': 40, 'fully_connected_size': 320, 'fully_connected_layers': 5, 'dropout_rate': 0.6493465192714587, 'weight_decay': 0.06877640509303769}. Best is trial 0 with value: 77.68909912109375.\u001b[0m\n",
      "\u001b[32m[I 2024-01-24 20:05:45,521]\u001b[0m Trial 3 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-24 21:28:20,883]\u001b[0m Trial 4 finished with value: 84.13840026855469 and parameters: {'learning_rate': 0.0017541127833188292, 'batch_size': 90, 'fully_connected_size': 128, 'fully_connected_layers': 2, 'dropout_rate': 0.6365300067786299, 'weight_decay': 0.006101988498535604}. Best is trial 4 with value: 84.13840026855469.\u001b[0m\n",
      "\u001b[32m[I 2024-01-24 21:39:26,449]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-24 22:32:18,708]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-24 23:42:04,769]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-24 23:53:55,471]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 00:09:38,460]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 00:20:28,827]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 00:48:00,709]\u001b[0m Trial 11 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 01:04:56,908]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 01:26:48,617]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 02:42:16,099]\u001b[0m Trial 14 finished with value: 48.247779083251956 and parameters: {'learning_rate': 0.0039503653672229785, 'batch_size': 80, 'fully_connected_size': 896, 'fully_connected_layers': 4, 'dropout_rate': 0.2504531532708078, 'weight_decay': 0.25361467318491004}. Best is trial 4 with value: 84.13840026855469.\u001b[0m\n",
      "\u001b[32m[I 2024-01-25 05:26:48,635]\u001b[0m Trial 15 finished with value: 79.70161437988281 and parameters: {'learning_rate': 3.7779552604151395e-05, 'batch_size': 40, 'fully_connected_size': 768, 'fully_connected_layers': 3, 'dropout_rate': 0.7940258484489011, 'weight_decay': 0.011115001976676427}. Best is trial 4 with value: 84.13840026855469.\u001b[0m\n",
      "\u001b[32m[I 2024-01-25 10:08:52,609]\u001b[0m Trial 16 finished with value: 84.14293670654297 and parameters: {'learning_rate': 0.00028882420141323036, 'batch_size': 20, 'fully_connected_size': 768, 'fully_connected_layers': 3, 'dropout_rate': 0.7856743983281198, 'weight_decay': 0.0025788516989160653}. Best is trial 16 with value: 84.14293670654297.\u001b[0m\n",
      "\u001b[32m[I 2024-01-25 10:56:20,384]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 14:42:31,789]\u001b[0m Trial 18 finished with value: 82.12135009765625 and parameters: {'learning_rate': 0.00026390771795991935, 'batch_size': 20, 'fully_connected_size': 128, 'fully_connected_layers': 4, 'dropout_rate': 0.5393186957293674, 'weight_decay': 0.04382108998343734}. Best is trial 16 with value: 84.14293670654297.\u001b[0m\n",
      "\u001b[32m[I 2024-01-25 15:52:52,466]\u001b[0m Trial 19 finished with value: 82.2528564453125 and parameters: {'learning_rate': 0.0021539310700911937, 'batch_size': 140, 'fully_connected_size': 384, 'fully_connected_layers': 3, 'dropout_rate': 0.6966495240175413, 'weight_decay': 0.007997692084601904}. Best is trial 16 with value: 84.14293670654297.\u001b[0m\n",
      "\u001b[32m[I 2024-01-25 16:21:51,430]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 16:54:05,449]\u001b[0m Trial 21 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 17:54:41,885]\u001b[0m Trial 22 finished with value: 51.74133987426758 and parameters: {'learning_rate': 0.0025425826356020027, 'batch_size': 140, 'fully_connected_size': 320, 'fully_connected_layers': 4, 'dropout_rate': 0.6226313723598789, 'weight_decay': 0.31183935944429986}. Best is trial 16 with value: 84.14293670654297.\u001b[0m\n",
      "\u001b[32m[I 2024-01-25 18:11:47,647]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 18:46:02,969]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 18:55:50,003]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 19:08:22,687]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 19:39:56,046]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2024-01-25 20:49:05,126]\u001b[0m Trial 28 finished with value: 65.31652450561523 and parameters: {'learning_rate': 0.0021097920774002764, 'batch_size': 80, 'fully_connected_size': 640, 'fully_connected_layers': 2, 'dropout_rate': 0.3684812968598801, 'weight_decay': 0.2784646259293541}. Best is trial 16 with value: 84.14293670654297.\u001b[0m\n",
      "\u001b[32m[I 2024-01-25 22:51:28,619]\u001b[0m Trial 29 finished with value: 97.98385772705078 and parameters: {'learning_rate': 0.0007500635378128876, 'batch_size': 120, 'fully_connected_size': 512, 'fully_connected_layers': 5, 'dropout_rate': 0.02202455035819756, 'weight_decay': 0.002662522701201958}. Best is trial 29 with value: 97.98385772705078.\u001b[0m\n",
      "\u001b[32m[I 2024-01-26 00:11:31,662]\u001b[0m Trial 30 finished with value: 93.95701293945312 and parameters: {'learning_rate': 0.0013982628214390617, 'batch_size': 120, 'fully_connected_size': 512, 'fully_connected_layers': 5, 'dropout_rate': 0.004592770643988881, 'weight_decay': 0.007108753447202203}. Best is trial 29 with value: 97.98385772705078.\u001b[0m\n",
      "\u001b[32m[I 2024-01-26 01:57:30,013]\u001b[0m Trial 31 finished with value: 81.31779479980469 and parameters: {'learning_rate': 0.00016999742032235292, 'batch_size': 80, 'fully_connected_size': 448, 'fully_connected_layers': 2, 'dropout_rate': 0.6304385050446437, 'weight_decay': 0.00013027540343048174}. Best is trial 29 with value: 97.98385772705078.\u001b[0m\n",
      "\u001b[32m[I 2024-01-26 02:07:01,946]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[33m[W 2024-01-26 03:57:31,696]\u001b[0m Trial 33 failed with parameters: {'learning_rate': 3.345689432184527e-05, 'batch_size': 120, 'fully_connected_size': 448, 'fully_connected_layers': 5, 'dropout_rate': 0.04051958377244162, 'weight_decay': 0.025289075771222594} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/cameron/DATA/Barcode_Envs/envs/venv_cg_gpu/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/action_functions.py\", line 862, in objective_with_custom_input\n",
      "    return bayesian.objective_code_classifier(trial, pipeline_inputs)\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/bayesian.py\", line 31, in objective_code_classifier\n",
      "    cross_val_scores = action_functions.train_code_classifier(\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/action_functions.py\", line 809, in train_code_classifier\n",
      "    cross_val_scores = trainer.train(\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/model_training/code_classification_trainer_GPU.py\", line 249, in train\n",
      "    loss.backward()\n",
      "  File \"/media/cameron/DATA/Barcode_Envs/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/media/cameron/DATA/Barcode_Envs/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2024-01-26 03:57:31,724]\u001b[0m Trial 33 failed with value None.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/main_cg_GPU.py\", line 42, in <module>\n",
      "    action_functions.bayesian_optimize_code_classifer(pipeline_inputs=pipeline_inputs)\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/action_functions.py\", line 864, in bayesian_optimize_code_classifer\n",
      "    study = bayesian.checkpoint_study(\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/bayesian.py\", line 85, in checkpoint_study\n",
      "    study.optimize(objective_function, n_trials=1)\n",
      "  File \"/media/cameron/DATA/Barcode_Envs/envs/venv_cg_gpu/lib/python3.10/site-packages/optuna/study/study.py\", line 425, in optimize\n",
      "    _optimize(\n",
      "  File \"/media/cameron/DATA/Barcode_Envs/envs/venv_cg_gpu/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 66, in _optimize\n",
      "    _optimize_sequential(\n",
      "  File \"/media/cameron/DATA/Barcode_Envs/envs/venv_cg_gpu/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 163, in _optimize_sequential\n",
      "    frozen_trial = _run_trial(study, func, catch)\n",
      "  File \"/media/cameron/DATA/Barcode_Envs/envs/venv_cg_gpu/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 251, in _run_trial\n",
      "    raise func_err\n",
      "  File \"/media/cameron/DATA/Barcode_Envs/envs/venv_cg_gpu/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/action_functions.py\", line 862, in objective_with_custom_input\n",
      "    return bayesian.objective_code_classifier(trial, pipeline_inputs)\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/bayesian.py\", line 31, in objective_code_classifier\n",
      "    cross_val_scores = action_functions.train_code_classifier(\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/utils/action_functions.py\", line 809, in train_code_classifier\n",
      "    cross_val_scores = trainer.train(\n",
      "  File \"/home/cameron/Dropbox (University of Michigan)/DL_training/model_training/code_classification_trainer_GPU.py\", line 249, in train\n",
      "    loss.backward()\n",
      "  File \"/media/cameron/DATA/Barcode_Envs/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/media/cameron/DATA/Barcode_Envs/envs/venv_cg_gpu/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\n",
    "    f\"python main_cg_GPU.py --action bayesian_optimize_code_classifier --pipeline_inputs {inputJSON}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
