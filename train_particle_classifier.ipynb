{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a785d-982f-43f1-b4b5-d44b17a38638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc408f27",
   "metadata": {},
   "source": [
    "# Train Particle Classifier(s): Control Panel\n",
    "### Author: CG\n",
    "\n",
    "This notebook succeeds the \"train_region_classifiers.ipynb\" notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee14e8",
   "metadata": {},
   "source": [
    "Here, we define a dictionary of input variables to initiate our particle detector training pipeline. This dictionary will be saved as a .json file and passed to the pipeline execution script, main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDict = {\n",
    "    \"sample_parent_directory\": \"/home/cameron/Dropbox (University of Michigan)/DL_training/data/classifier_training_samples\",\n",
    "    # Where to save the timestamped model\n",
    "    \"model_save_parent_directory\": \"data/models/code\",\n",
    "    # Specify codes to separately train the region detector on\n",
    "    # Must be in format \"(NUMBER)\" as per our experimental convention for naming codes\n",
    "    \"code_list\": [\"(1)\", \"(2)\", \"(3)\", \"(4)\", \"(5)\"],\n",
    "    # Fraction of total dataset to allocate as the test dataset for model predictive ability\n",
    "    \"test_size\": 0.20,\n",
    "    # Log via TensorBoard\n",
    "    \"log\": True,\n",
    "    # Printing verbosity\n",
    "    \"verbose\": True,\n",
    "    # Timestamp corresponding to this particular hyperparameter optimization run,\n",
    "    \"timestamp\": datetime.now().strftime(\"%m_%d_%y_%H:%M\"),\n",
    "    \"strat_kfold\": {\n",
    "        \"activate\": True,\n",
    "        # Num. of folds\n",
    "        \"num_folds\": 5,\n",
    "        # Controls the splitting of data in a reproducible way if the same seed is used\n",
    "        \"random_state\": 100,\n",
    "    },\n",
    "    # Save checkpointed version of model every n epochs\n",
    "    \"save_every_n\": 1,\n",
    "    # Hyperparameters\n",
    "    # Batch Size\n",
    "    \"batch_size\": 192,\n",
    "    # Learning Rate\n",
    "    \"lr\": 3e-4,\n",
    "    # Size of each fully-connected layer\n",
    "    \"fc_size\": 64,\n",
    "    # Number of fully-connected layers,\n",
    "    \"fc_num\": 2,\n",
    "    # Dropout rate to prevent overfitting\n",
    "    \"dropout_rate\": 0.3\n",
    "}\n",
    "\n",
    "# File save name\n",
    "inputJSON = \"train_particle_detector.json\"\n",
    "# Save\n",
    "with open(inputJSON, \"w\") as jsonFile:\n",
    "    json.dump(inputDict, jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9cbd7a",
   "metadata": {},
   "source": [
    "## Train Particle Detectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0603d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.system(\n",
    "    f\"python main_cg_GPU.py --action train_code_classifier --pipeline_inputs {inputJSON}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df1ca61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
